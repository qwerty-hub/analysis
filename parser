import requests_html
import sqlite3
from bs4 import BeautifulSoup

#...на всякий случай вдруг пригодится
class Proxy(object):
    def __init__(self, address, port, country, speed, delay):
        self.address = address
        self.port = port
        self.country = country
        self.speed = speed
        self.delay = delay

#...создание базы данных Sqlite
#...!добавить недостающие атрибуты
def createDb():
    connection = sqlite3.connect('proxy.db')
    cursor = connection.cursor()
    cursor.execute('''CREATE TABLE IF NOT EXISTS ProxyList
                    (Address TEXT UNIQUE, Port TEXT)''')
    connection.commit()
    connection.close()

#...вывод существующей базы данны
def printDb():
    connection = sqlite3.connect('proxy.db')
    cursor = connection.cursor()
    records = cursor.execute("SELECT * FROM ProxyList")
    print('|...Address...|...port.....|')
    for elem in cursor.fetchall():
        print(elem[0], '   ', elem[1])
    connection.commit()
    connection.close()


#...парсинг сайта https://free-proxy-list.net
#на странице содержит 20 записей, которые обновляются каждые 10 минут
def parsing_proxy1():
    connection = sqlite3.connect('proxy.db')
    cursor = connection.cursor()
    proxylist = list()
    session = requests_html.HTMLSession()
    r = session.get('https://free-proxy-list.net/')
    r.html.render()
    for i in range(1, 21):
        address = r.html.xpath('/html/body/section[1]/div/div[2]/div/div[2]/div/table/tbody/tr[{}]/td[1]/text()'.format(i))[0]
        port = r.html.xpath('/html/body/section[1]/div/div[2]/div/div[2]/div/table/tbody/tr[{}]/td[2]/text()'.format(i))[0]
        proxylist.append([address,port])
    cursor.executemany("INSERT INTO ProxyList (Address, port) VALUES (?, ?)", proxylist)
    connection.commit()
    connection.close()

#...парсинг сайта http://foxtools.ru
#содержит 3 страницы ссылка на страницу http://foxtools.ru/Proxy?page='номер страницы'
def parsing_proxy2():
    connection = sqlite3.connect('proxy.db')
    cursor = connection.cursor()
    proxylist = list()
    session = requests_html.HTMLSession()
    for i in ['1','2','3']:
        r = session.get('http://foxtools.ru/Proxy?page='+i)
        soup = BeautifulSoup(r.text, 'lxml')
        line = soup.find('table', id='theProxyList').find('tbody').find_all('tr')
        for tr in line:
            td = tr.find_all('td')
            address = td[1].text
            port = td[2].text
            proxylist.append([address,port])
    cursor.executemany("INSERT INTO ProxyList (Address, port) VALUES (?, ?)", proxylist)
    connection.commit()
    connection.close()

    #...парсинг сайта https://spys.one/en/free-proxy-list/
#имеет несколько разделов с прокси
#стандартно отображает 30 записей, на сайте можно выбрать 500
#...?как сделать загрузку сайта с 500 записями
#...!не отображает порт
def parsing_proxy3():
    connection = sqlite3.connect('proxy.db')
    cursor = connection.cursor()
    proxylist = list()
    session = requests_html.HTMLSession()
    r = session.get('https://spys.one/en/free-proxy-list/')
    soup = BeautifulSoup(r.text, 'lxml')
    line = soup.find('body').find_all('table')[1].find_all('tr')[3].find('table').find_all('tr')
    print(line[2].find_all('td')[0].find('font').text)
    print(line[2])
    '''
    for i in range[1:16]:
        line[2*i].find_all('td')[1].find('font')
        td = tr.find_all('td')
        address = td[1].text
        port = td[2].text
        proxylist.append([address,port])
    cursor.executemany("INSERT INTO ProxyList (Address, port) VALUES (?, ?)", proxylist)
    connection.commit()
    connection.close()
    '''
#парсинг сайта https://awmproxy.net/
#доступ по captcha
#содержит 8057 прокси
#пробуем сохранять сайт в ручную и потом парсить



#createDb()
#parsing_proxy1()
#parsing_proxy2()
parsing_proxy3()
#printDb()
